<!DOCTYPE html>
<html>
  <head>
    <title>Deep Learning Lectures</title>
    <meta charset="utf-8">
    <style>
     .left-column {
       width: 50%;
       float: left;
     }
     .reset-column {
       overflow: auto;
        width: 100%;
     }
     .small { font-size: 0.2em; }
     .right-column {
       width: 50%;
       float: right;
     }
     .right{
       float:right;
     }
     .left{
       float:left;
     }
     .footnote {
        position: absolute;
        bottom: 2em;
        margin: 0em 2em;
     }
     .red { color: #ee1111; }
     .grey { color: #bbbbbb; }
      </style>
    <link rel="stylesheet" type="text/css" href="slides.css">
  </head>
  <body>
    <textarea id="source">
class: center, middle

# Embeddings and Natural Language Processing

Roman Yurchak

Course material by Charles Ollion - Olivier Grisel

.affiliations[
  ![Heuritech](images/logo heuritech v2.png)
  ![Inria](images/inria-logo.png)
  ![UPS](images/Logo_Master_Datascience.png)
]


---
# Outline

### Embeddings

--

### Dropout Regularization

--

### NLP: Classification and word representation

--

### NLP: Word2Vec

---
class: middle, center

# Embeddings

---
# Symbolic variable

- Text: characters, words, bigrams...

- <span style="color:#cccccc">Recommender Systems: item ids, user ids</span>

- <span style="color:#cccccc">Any categorical descriptor: tags, movie genres, visited URLs, skills on a resume, product categories...</span>

---
# Symbolic variable

- Text: characters, words, bigrams...

- Recommender Systems: item ids, user ids 

- <span style="color:#cccccc">Any categorical descriptor: tags, movie genres, visited URLs, skills on a resume, product categories...</span>

---
# Symbolic variable

- Text: characters, words, bigrams...

- Recommender Systems: item ids, user ids 

- Any categorical descriptor: tags, movie genres, visited URLs, skills on a resume, product categories...

--

### Notation:

.center[
### Symbol $s$ in vocabulary $V$
]

---
# One-hot representation

$$onehot(\text{'salad'}) = [0, 0, 1, ..., 0] \in \\{0, 1\\}^{|V|}$$


.center[
          <img src="images/word_onehot.svg" style="width: 400px;" />
]

--

<br/>

- Sparse, discrete, large dimension $|V|$
- Each axis has a meaning
- Symbols are equidistant from each other:

.center[euclidean distance = $\sqrt{2}$]

---
# Embedding

$$embedding(\text{'salad'}) = [3.28, -0.45, ... 7.11] \in \mathbb{R}^d$$

--

<br/> 

- Continuous and dense
- Can represent a huge vocabulary in low dimension, typically: $d \in \\{16, 32, ..., 4096\\}$
- Axis have no meaning _a priori_
- Embedding metric can capture semantic distance

--

<br/>

**Neural Networks compute transformations on continuous vectors**

???
 _Yann Le Cun_ : "compute symbolic operations in algebraic space makes it possible to optimize via gradient descent"

---
# Implementation with Keras

Size of vocabulary $n = |V|$, size of embedding $d$

```py
# input: batch of integers
Embedding(output_dim=d, input_dim=n, input_length=1)
# output: batch of float vectors
```

--
- Equivalent to one-hot encoding multiplied by a weight matrix $\mathbf{W} \in \mathbb{R}^{n \times d}$:

$$embedding(x) = onehot(x) . \mathbf{W}$$

--
- $\mathbf{W}$ is typically **randomly initialized**, then **tuned by backprop**

--
- $\mathbf{W}$ are trainable parameters of the model

---
# Distance and similarity in Embedding space

.left-column[
### Euclidean distance

$d(x,y) = || x - y ||_2$

- Simple with good properties
- Dependent on norm (embeddings usually unconstrained)
]

--

.right-column[
### Cosine similarity

$cosine(x,y) = \frac{x \cdot y}{||x|| \cdot ||y||}$

- Angle between points, regardless of norm
- $cosine(x,y) \in (-1,1)$
- Expected cosine similarity of random pairs of vectors is $0$ 

]

---
# Distance and similarity in Embedding space

If $x$ and $y$ both have unit norms:

$$|| x - y ||_2^2 = 2 \cdot (1 - cosine(x, y))$$

--
or alternatively:

$$cosine(x, y) = 1 - \frac{|| x - y ||_2^2}{2}$$

--

Alternatively, dot product (unnormalized) is used in practice as a pseudo similarity 

---
# Visualizing Embeddings

- Visualizing requires a projection in 2 or 3 dimensions
- Objective: visualize which embedded symbols are similar

--

### PCA

- Limited by linear projection, embeddings usually have complex high dimensional structure

--

### t-SNE

          <small>
              Visualizing data using t-SNE, L van der Maaten, G Hinton, _The Journal of Machine Learning Research_, 2008 <br/>
          </small>


???
https://colah.github.io/posts/2014-10-Visualizing-MNIST/
---
# t-Distributed Stochastic Neighbor Embedding


 - Unsupervised, low-dimension, non-linear projection
 - Optimized to keep relative distance between nearest neighbors

<br/>

--

### t-SNE projection is non deterministic (depends on initialization)
 - Critical parameter: perplexity, usually set to 20, 30
 - See http://distill.pub/2016/misread-tsne/

---
#Example word vectors

.center[
          <img src="images/tsne_words.png" style="width: 630px;" /><br/>
          <small>excerpt from work by J. Turian on a model trained by R. Collobert et al. 2008</small>
]

---
# Visualizing Mnist

.center[
          <img src="images/mnist_tsne.jpg" style="width: 400px;" />
]

---

class: middle, center

# Dropout Regularization

---

# Regularization

### Size of the embeddings

--

### Depth of the network

--

### $L_2$ penalty on embeddings

--

### Dropout

- Randomly set activations to $0$ with probability $p$
- Bernoulli mask sampled for a forward pass / backward pass pair
- Typically only enabled at training time

---
# Dropout

.center[
          <img src="images/dropout.png" style="width: 680px;" />
]

.footnote.small[Dropout: A Simple Way to Prevent Neural Networks from Overfitting,
Srivastava et al., _Journal of Machine Learning Research_ 2014]

---
# Dropout

### Interpretation

- Reduces the network dependency to individual neurons
- More redundant representation of data

### Ensemble interpretation

- Equivalent to training a large ensemble of shared-parameters, binary-masked models
- Each model is only trained on a single data point

---
#Dropout

.center[
          <img src="images/dropout_traintest.png" style="width: 600px;" /><br/>

]
          <br/>

At test time, multiply weights by $p$ to keep same level of activation

.footnote.small[Dropout: A Simple Way to Prevent Neural Networks from Overfitting,
Srivastava et al., _Journal of Machine Learning Research_ 2014]

---
# Overfitting Noise

.center[
          <img src="images/dropout_curves_1.svg" style="width: 600px;" /><br/>
]

???
This dataset has few samples and ~10% noisy labels.
The model is seriously overparametrized (3 wide hidden layers).
The training loss goes to zero while the validation stops decreasing
after a few epochs and starts increasing again: this is a serious case
of overfitting.

---
# A bit of Dropout

.center[
          <img src="images/dropout_curves_2.svg" style="width: 600px;" /><br/>
]

???
With dropout the training speed is much slower and the training loss
has many random bumps caused by the additional variance in the updates
of SGD with dropout.

The validation loss on the other hand stays closer to the training loss
and can reach a slightly lower level than the model without dropout:
overfitting is reduced but not completely solved.

---
# Too much: Underfitting

.center[
          <img src="images/dropout_curves_3.svg" style="width: 600px;" /><br/>
]

---
# Implementation with Keras

<br/>
<br/>
```py
model = Sequential()
model.add(Dense(hidden_size, input_shape, activation='relu'))
*model.add(Dropout(p=0.5))
model.add(Dense(hidden_size, activation='relu'))
*model.add(Dropout(p=0.5))
model.add(Dense(output_size, activation='softmax'))
```

???

In practice, activations are multiplied by $\frac{1}{p}$ at train time, test time is unchanged

---

# Batch Normalization

Normalize activations in each **mini-batch** before activation function:
**speeds up** and **stabilizes** training (less dependent on init)

.footnote.small[
Ioffe, Sergey, and Christian Szegedy. "Batch normalization:
Accelerating deep network training by reducing internal covariate
shift." ICML 2015
]

--
.center[
<img src="images/batchnorm.png" style="width: 450px;" />
]

???
Used successfully in conjunction to residual connection.

---
# Batch Normalization

At **inference time**, use average and standard deviation computed on
**the whole dataset** instead of batch

--

Widely used in **ConvNets**, but requires the mini-batch to be large
enough to compute statistics in the minibatch.

--

In **Keras**: use it as a layer, before activation	
	
```python
x = Convolution2D(64, 1, 1)(input_tensor)
x = BatchNormalization()(x)
x = Activation('relu')(x)
```

--

- Introduces new parameters: `2 x size_of_activation` (here 128)

--
- Keras keeps track of a **running average/std** of activations for
  inference, no need to have specific inference code.


---
## Natural Language Processing

.center[
<img src="images/nlp.png" style="width: 550px;" />
]

---
## Natural Language Processing

- Sentence/Document level Classification (topic, sentiment)

- Topic modeling (LDA, ...)

--

- Translation

--

- Chatbots / dialogue systems / assistants (Alexa, ...)

--

- Summarization

---
# Recommended reading

*A Primer on Neural Network Models for Natural Language Processing* by
Yoav Goldberg

.center[
http://u.cs.biu.ac.il/~yogo/nnlp.pdf
]

--

Useful open source projects

.center[
<img src="images/logolibs.png" style="width: 500px;" />
]

---
---
class: middle, center

# Word Representation and Word2Vec

---
# Word representation

Words are indexed and represented as 1-hot vectors

--

Large Vocabulary of possible words $|V|$

--

Use of **Embeddings** as inputs in all Deep NLP tasks

--

Word embeddings usually have dimensions 50, 100, 200, 300

---
# Supervised Text Classification

.center[
<img src="images/fasttext.svg" style="width: 500px;" />
]

.footnote.small[
Joulin, Armand, et al. "Bag of tricks for efficient text classification." FAIR 2016
]

???
Question: shape of embeddings if hidden size is H

--

$\mathbf{E}$ embedding (linear projection) .right.red[`|V| x H`]

--

Embeddings are averaged .right[hidden activation size: .red[`H`]]

--

Dense output connection $\mathbf{W}, \mathbf{b}$ .right.red[`H x K`]

--

Softmax and **cross-entropy** loss


---
# Supervised Text Classification

.center[
<img src="images/fasttext.svg" style="width: 500px;" />
]

.footnote.small[
Joulin, Armand, et al. "Bag of tricks for efficient text classification." FAIR 2016
]

<br/>

- Very efficient (**speed** and **accuracy**) on large datasets

--
- State-of-the-art (or close to) on several classification, when adding **bigrams/trigrams**

--
- Little gains from depth

---
# Transfer Learning for Text

Similar to image: can we have word representations that are generic
enough to **transfer** from one task to another?

--

**Unsupervised / self-supervised** learning of word representations

--

**Unlabelled** text data is almost infinite:
  - Wikipedia dumps
  - Project Gutenberg
  - Social Networks
  - Common Crawl

---
# Word Vectors

.center[
<img src="images/tsne_words.png" style="width: 630px;" />
]

.footnote.small[
excerpt from work by J. Turian on a model trained by R. Collobert et al. 2008
]

???
Question: what distance to use in such a space
---
# Word2Vec

.center[
<img src="images/most_sim.png" style="width: 500px;" />
]

.footnote.small[
Colobert et al. 2011, Mikolov, et al. 2013
]

--

<br/>

### Compositionality

.center[
<img src="images/sum_wv.png" style="width: 700px;" />
]

---
# Word Analogies

.center[
<img src="images/capitals.png" style="width: 450px;" />
]

.footnote.small[
Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." NIPS 2013
]

--

- Linear relations in Word2Vec embeddings

--
- Many come from text structure (e.g. Wikipedia)

---
# Self-supervised training

Distributional Hypothesis (Harris, 1954):
*“words are characterised by the company that they keep”*

Main idea: learning word embeddings by **predicting word contexts**

.footnote.small[
Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." NIPS 2013
]

--

Given a word e.g. “carrot” and any other word $w \in V$ predict
probability $P(w|\text{carrot})$ that $w$ occurs in the context of
“carrot”.

--

- **Unsupervised / self-supervised**: no need for class labels.
- (Self-)supervision comes from **context**.
- Requires a lot of text data to cover rare words correctly.

???
How to train fastText like model on this?
---
# Word2Vec: CBoW

CBoW: representing the context as **Continuous Bag-of-Word**

Self-supervision from large unlabeled corpus of text: *slide* over an **anchor word** and its **context**:

.center[
<img src="images/words.svg" style="width: 500px;" />
]

.footnote.small[
Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." NIPS 2013
]

---
# Word2Vec: CBoW

CBoW: representing the context as **Continuous Bag-of-Word**

Self-supervision from large unlabeled corpus of text: *slide* over an **anchor word** and its **context**:

.center[
<img src="images/word2vec_words.svg" style="width: 500px;" />
]

.footnote.small[
Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." NIPS 2013
]

???
Question: dim of output embedding vs dim of input embedding
---
# Word2Vec: Details

<br/>

.center[
<img src="images/word2vec.svg" style="width: 500px;" />
]

.footnote.small[
Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." NIPS 2013
]

- Similar as supervised CBoW (e.g. fastText) with |V| classes

--
- Use **Negative Sampling**: sample *negative* words at random instead of computing the full softmax. See: <small> http://sebastianruder.com/word-embeddings-softmax/index.html</small>

--
- Large impact of **context size**

???

Softmax is too computationally intensive to be practical: the
normalization term involves a sum over the full vocabulary of
cardinality |V| >> 10000 at each gradient step.

Negative Sampling uses k=5 negative words sampled at random instead.
This is not accurate enough to estimate
`p(x_t|x_{t-2}, x_{t-1}, x_{t+1}, x_t{t+2})`
accuractely but it's a good enough approximation to train a
useful word embedding parameters.

---
# Word2Vec: Skip Gram
a
<br/>

.center[
<img src="images/word2vec_skipgram.svg" style="width: 500px;" />
]

<br/>

- Given the central word, predict occurence of other words in its context.

--
- Widely used in practice

--
- Again **Negative Sampling** is used as a cheaper alternative to full softmax.

---
# Evaluation and Related methods

Always difficult to evaluate unsupervised tasks

- WordSim (Finkelstein et al.)
- SimLex-999 (Hill et al.)
- Word Analogies (Mikolov et al.)

--

<br/>

Other popular method: **GloVe** (Socher et al.) http://nlp.stanford.edu/projects/glove/

.footnote.small[
Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global Vectors for Word Representation." EMNLP. 2014
]
---
# Take Away on Embeddings

**For text applications, inputs of Neural Networks are Embeddings**

--

- If **little training data** and a wide vocabulary not well
  covered by training data, use **pre-trained self-supervised embeddings**
  (transfer learning from Glove, word2vec or fastText embeddings)

--
- If **large training data** with labels, directly learn
  task-specific embedding with methods such as **fastText in
  supervised mode**.

--
- These methods use **Bag-of-Words** (BoW): they **ignore the order** in
  word sequences

--
- Depth &amp; non-linear activations on hidden layers are not that useful for
  BoW text classification.

--

**Deep Learning in NLP** is less mature than for other domains such as
computer vision and speech recognition.


    </textarea>
    <style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
		     all[i].SourceElement().parentNode.className += ' has-jax';
		     }
		     });
		     </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true
      });
    </script>
  </body>
</html>
